{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, layers, Model\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OCRDataLoader():\n",
    "    def __init__(self,train_path,\n",
    "                 batch_size=64, repeat=1):\n",
    "        \"\"\" Loads the tf.data.Dataset \n",
    "        Parameters:\n",
    "            -annotation_path :  Path of image folder to be loaded in dataset\n",
    "            -labels_path :  path of labels.txt file\n",
    "            -image_width : image width \n",
    "            -table_path  : path of the table.txt file\n",
    "            -batch_size  : Batch size \n",
    "            -repeat      : The number of times we want data to repeat. e.g if repeat 1 data is iterted once.\n",
    "            \n",
    "        \"\"\"\n",
    "        images_path=os.path.join(train_path,\"images\")\n",
    "        labels_path=os.path.join(train_path,\"labels\")\n",
    "        img_paths, label_paths = read_img_paths_and_labels(\n",
    "            images_path, \n",
    "            labels_path)\n",
    "        self.batch_size = batch_size\n",
    "        self.size = len(img_paths)\n",
    "\n",
    "\n",
    "        ds = tf.data.Dataset.from_tensor_slices((img_paths, label_paths))\n",
    "        ds = ds.map(self._decode_and_resize)\n",
    "        ds = ds.apply(tf.data.experimental.ignore_errors())\n",
    "        ds = ds.batch(batch_size).repeat(repeat)\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        self.dataset = ds\n",
    "\n",
    "    def _decode_and_resize(self, image_filename, label_filename):\n",
    "        \"\"\" Reads and resize image (64,image_width)\n",
    "            Parameters:\n",
    "                -filename : Name of file.\n",
    "                -label   : label of that image.\n",
    "                \n",
    "            returns image tensor and label.\n",
    "        \"\"\"\n",
    "        image = tf.io.read_file(image_filename)\n",
    "        image = tf.io.decode_png(image, channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        print(\"image:\\t\")\n",
    "        print(image.shape)\n",
    "        image = tf.image.resize(image, (2048,1024))\n",
    "        label = tf.io.read_file(label_filename)\n",
    "        label = tf.io.decode_png(label, channels=3)\n",
    "        print(label.shape)\n",
    "        label = tf.image.resize(label, (2048,1024))\n",
    "        label = tf.cast((label[:,:,0]>100), tf.int32)\n",
    "#         label = tf.image.convert_image_dtype(label, tf.float32)\n",
    "        label = tf.one_hot(label, 2) #change to n_classes\n",
    "        return image, label\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Return tf.data.Dataset.\"\"\"\n",
    "        return self.dataset\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "def read_img_paths_and_labels(images_path, labels_path):\n",
    "    \"\"\"return image filenames  and respective labels.\n",
    "       Parameters:\n",
    "           -data_path : Path of the folder of images.\n",
    "           -labels_path : path of label.txt\n",
    "    \n",
    "    \"\"\"\n",
    "    img_path=os.listdir(images_path)\n",
    "    img_path.sort()\n",
    "    img_path=[str(os.path.join(images_path,i)) for i in img_path]\n",
    "    label_path=os.listdir(labels_path)\n",
    "    label_path.sort()\n",
    "    label_path=[str(os.path.join(labels_path,i)) for i in label_path]\n",
    "    print(img_path,label_path)\n",
    "    \n",
    "    return img_path, label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/images/0.png', '/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/images/1.png', '/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/images/2.png', '/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/images/3.png', '/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/images/4.png'] ['/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/labels/0.png', '/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/labels/1.png', '/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/labels/2.png', '/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/labels/3.png', '/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/labels/4.png']\n",
      "image:\t\n",
      "(None, None, 3)\n",
      "(None, None, 3)\n"
     ]
    }
   ],
   "source": [
    "x=OCRDataLoader(\"/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/\",batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter(x,y):\n",
    "        x=tf.image.adjust_brightness(x,150)\n",
    "        x=tf.image.adjust_contrast(x,100)\n",
    "        x=tf.image.adjust_hue(x,100)\n",
    "        x=tf.image.adjust_saturation(x,200)\n",
    "        x=tf.image.rot90(x)\n",
    "\n",
    "        return (x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class augmentation():\n",
    "    def __init__(self,ds,prob=0.02):\n",
    "        self.dataset=ds\n",
    "        self.prob=prob\n",
    "    def jitter(self,x,y):\n",
    "        if np.random.choice(np.linspace(start=0.0,stop=1.0,num=100))<=self.prob:\n",
    "            x=tf.image.adjust_brightness(x,np.random.choice(np.linspace(start=0.0,stop=1.0,num=100)))\n",
    "            x=tf.image.adjust_contrast(x,np.random.choice(np.linspace(start=0.0,stop=1.0,num=100)))\n",
    "            x=tf.image.adjust_hue(x,np.random.choice(np.linspace(start=0.0,stop=1.0,num=100)))\n",
    "            x=tf.image.adjust_saturation(x,np.random.choice(np.linspace(start=0.0,stop=1.0,num=100)))\n",
    "        return (x,y)\n",
    "    def __call__(self):\n",
    "        return self.dataset().map(self.jitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=augmentation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([   2 2048 1024    3], shape=(4,), dtype=int32) tf.Tensor([   2 2048 1024    2], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i ,j in d():\n",
    "    print(tf.shape(i),tf.shape(j))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in x():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2048, 1024, 3), dtype=float32, numpy=\n",
       "array([[[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]]], dtype=float32)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-174450de7969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 7200x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig=plt.figure(figsize=(100, 100))\n",
    "row=1\n",
    "itter=1\n",
    "for i,j in a:\n",
    "    fig.add_subplot(10,2,itter)\n",
    "    plt.imshow(i[0])\n",
    "    fig.add_subplot(10,2,itter+1)\n",
    "    plt.imshow(j[0][:,:,1])\n",
    "    \n",
    "    itter+=2\n",
    "plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bottleneck(input_tensor,intial,filter_,stride=(1,1),connect=True,is_first=False):\n",
    "    x=layers.Conv2D(filters=filter_,kernel_size=1,strides=(1,1),activation='relu')(input_tensor)\n",
    "    x=layers.Conv2D(filters=filter_,kernel_size=3,strides=stride,padding=\"same\",activation='relu')(x)\n",
    "    x=layers.Conv2D(filters=intial,kernel_size=1,strides=(1,1))(x)\n",
    "    \n",
    "#     print(x.shape)\n",
    "#     print(input_tensor.shape)\n",
    "    if connect:\n",
    "        if is_first:\n",
    "#             print(\"here\")\n",
    "            input_tensor=layers.Conv2D(filters=intial,kernel_size=1,strides=(1,1))(input_tensor)\n",
    "#             print(\"inside\", input_tensor.shape)\n",
    "        print(x.shape,input_tensor.shape)\n",
    "        return x+input_tensor\n",
    "    else:\n",
    "        print(\"not Conected\")\n",
    "        print(x.shape)\n",
    "    return x\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling(input_layer,concat_layer,filters):\n",
    "    x=layers.Conv2DTranspose(input_layer.shape[-1],kernel_size=1, strides=(2,2))(input_layer)\n",
    "    #print(concat_layer.shape, x.shape)\n",
    "    x=tf.concat([concat_layer,x],3)\n",
    "    #print(x.shape)\n",
    "    x=layers.Conv2D(filters,kernel_size=3,padding='same',activation='relu')(x)\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dh(num_clases):\n",
    "    input_tensor=Input(shape=(2048,1024,3))\n",
    "    x1=layers.Conv2D(filters=64,kernel_size=7,strides=(2,2), padding=\"same\")(input_tensor)\n",
    "    x2=layers.MaxPool2D(strides=(2,2))(x1)\n",
    "    \n",
    "    \n",
    "    x3=bottleneck(x2,256,64,connect=True,is_first=True)\n",
    "    x4=bottleneck(x3,256,64)\n",
    "    x5=bottleneck(x4,256,64,(2,2),False)\n",
    "    \n",
    "    \n",
    "    x6=bottleneck(x5,512,128,is_first=True)\n",
    "    x7=bottleneck(x6,512,128)\n",
    "    x8=bottleneck(x7,512,128)\n",
    "    x9=bottleneck(x8,512,128,(2,2),False)\n",
    "    \n",
    "    x10=bottleneck(x9,1024,256,is_first=True)\n",
    "    x11=bottleneck(x10,1024,256)\n",
    "    x12=bottleneck(x11,1024,256)\n",
    "    x13=bottleneck(x12,1024,256)\n",
    "    x14=bottleneck(x13,1024,256)\n",
    "    x15=bottleneck(x14,1024,256,(2,2),False)\n",
    "    \n",
    "    \n",
    "    x16=bottleneck(x15,2048,512,is_first=True)\n",
    "    x17=bottleneck(x16,2048,512)\n",
    "    x18=bottleneck(x17,2048,512)\n",
    "    \n",
    "    \n",
    "    y1=layers.Conv2D(512,kernel_size=1,strides=(1,1),padding=\"same\")(x14)\n",
    "    \n",
    "    z1=layers.Conv2D(512,kernel_size=1,strides=(1,1),padding=\"same\")(x18)\n",
    "    \n",
    "    z2=upsampling(z1,y1,512)\n",
    "    z3=upsampling(z2,x8,256)\n",
    "    z4=upsampling(z3,x4,128)\n",
    "    z5=upsampling(z4,x1,64)\n",
    "    z6=upsampling(z5,input_tensor,32)\n",
    "    \n",
    "    z7=layers.Conv2D(filters=num_clases,kernel_size=1,padding='same')(z6)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Model(inputs=input_tensor,outputs=z7,name=\"dh\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 512, 256, 256) (None, 512, 256, 256)\n",
      "(None, 512, 256, 256) (None, 512, 256, 256)\n",
      "not Conected\n",
      "(None, 256, 128, 256)\n",
      "(None, 256, 128, 512) (None, 256, 128, 512)\n",
      "(None, 256, 128, 512) (None, 256, 128, 512)\n",
      "(None, 256, 128, 512) (None, 256, 128, 512)\n",
      "not Conected\n",
      "(None, 128, 64, 512)\n",
      "(None, 128, 64, 1024) (None, 128, 64, 1024)\n",
      "(None, 128, 64, 1024) (None, 128, 64, 1024)\n",
      "(None, 128, 64, 1024) (None, 128, 64, 1024)\n",
      "(None, 128, 64, 1024) (None, 128, 64, 1024)\n",
      "(None, 128, 64, 1024) (None, 128, 64, 1024)\n",
      "not Conected\n",
      "(None, 64, 32, 1024)\n",
      "(None, 64, 32, 2048) (None, 64, 32, 2048)\n",
      "(None, 64, 32, 2048) (None, 64, 32, 2048)\n",
      "(None, 64, 32, 2048) (None, 64, 32, 2048)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Input,layers,Model\n",
    "import tensorflow as tf\n",
    "a=dh(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl=OCRDataLoader(\"/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/\",batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        print(logits.shape)\n",
    "        logit_length = tf.fill([tf.shape(logits)[0]], tf.shape(logits)[1])\n",
    "        print(logit_length)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(y,logits)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss ,logits,logit_length\n",
    "\n",
    "def train(model, optimizer, dl, epoch,log_freq=10):\n",
    "    avg_loss = tf.keras.metrics.Mean(name=\"loss\", dtype=tf.float32)\n",
    "    steps = round(len(dl) / batch_size) + 1\n",
    "    \n",
    "    for i, (x, y) in enumerate(dl()):\n",
    "        loss,logits,logit_length = train_one_step(model, optimizer, x, y)\n",
    "        avg_loss.update_state(loss)\n",
    "        if tf.equal(optimizer.iterations % log_freq, 0):\n",
    "            print(\"{:.0%} Training... loss: {:.6f}  \".format(\n",
    "                (i + 1) / steps, avg_loss.result()), end='')\n",
    "            tf.summary.scalar(\"loss\", avg_loss.result(), \n",
    "                              step=optimizer.iterations)\n",
    "            avg_loss.reset_states()\n",
    "            if i < steps - 1:\n",
    "                print(end='\\r')\n",
    "        if epoch%10==0 and i%50==0:\n",
    "            \n",
    "            decoded, neg_sum_logits = tf.nn.ctc_greedy_decoder(\n",
    "        inputs=tf.transpose(logits, perm=[1, 0, 2]),\n",
    "        sequence_length=logit_length,\n",
    "        merge_repeated=True)\n",
    "            #print(tf.sparse.to_dense(decoded[0], default_value=0).numpy()\n",
    "      \n",
    "    return df\n",
    "\n",
    "def val_one_step(model, x, y):\n",
    "    logits = model(x, training=False)\n",
    "    logit_length = tf.fill([tf.shape(logits)[0]], tf.shape(logits)[1])\n",
    "    loss = tf.nn.ctc_loss(\n",
    "        labels=y,\n",
    "        logits=logits,\n",
    "        label_length=None,\n",
    "        logit_length=logit_length,\n",
    "        logits_time_major=False,\n",
    "        blank_index=-1)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    decoded, neg_sum_logits = tf.nn.ctc_greedy_decoder(\n",
    "        inputs=tf.transpose(logits, perm=[1, 0, 2]),\n",
    "        sequence_length=logit_length,\n",
    "        merge_repeated=True)\n",
    "    return loss, decoded\n",
    "\n",
    "def val(model, dl, step):\n",
    "    avg_loss = tf.keras.metrics.Mean(name=\"loss\", dtype=tf.float32)\n",
    "    steps = round(len(dl) / batch_size) + 1\n",
    "    total_cnt = 0\n",
    "    for i, (x, y) in enumerate(dl()):\n",
    "        loss, decoded = val_one_step(model, x, y)\n",
    "        avg_loss.update_state(loss)\n",
    "        cnt = map_and_count(decoded[0], y, dl.inv_table, dl.blank_index)\n",
    "        total_cnt += cnt\n",
    "        t.append(tf.sparse.to_dense(y,default_value=0).numpy())\n",
    "        p.append(tf.sparse.to_dense(decoded[0],default_value=0).numpy())\n",
    "        t_.append(map_to_chars(y,val_dl.inv_table))\n",
    "        p_.append(map_to_chars(decoded[0],val_dl.inv_table))\n",
    "    \n",
    "    \n",
    "    accuracy = total_cnt / len(dl)\n",
    "    print(\"Total: {}, Accuracy: {:.2%}\".format(total_cnt, accuracy))\n",
    "    tf.summary.scalar(\"loss\", avg_loss.result(), step=step)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy, step=step)\n",
    "    avg_loss.reset_states()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([1,1,0,1,1]).astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits=np.array([1,0,1,0,1]).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softmax_cross_entropy_with_logits_v2() got an unexpected keyword argument 'weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-4ae6de668fc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: softmax_cross_entropy_with_logits_v2() got an unexpected keyword argument 'weights'"
     ]
    }
   ],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(y,logits,weights=(1,700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=tf.nn.softmax_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=c(y,logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=bool, numpy=True>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar(\"loss\", avg_loss.result(), \n",
    "               `               step=optimizer.iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl=None\n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "type(time.asctime())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at Thu Oct  1 15:11:21 2020\n",
      "(None, 128, 64, 512) (None, 128, 64, 512)\n",
      "(None, 128, 64, 1024)\n",
      "(None, 256, 128, 512) (None, 256, 128, 512)\n",
      "(None, 256, 128, 1024)\n",
      "(None, 512, 256, 256) (None, 512, 256, 256)\n",
      "(None, 512, 256, 512)\n",
      "(None, 1024, 512, 64) (None, 1024, 512, 128)\n",
      "(None, 1024, 512, 192)\n",
      "(None, 2048, 1024, 3) (None, 2048, 1024, 64)\n",
      "(None, 2048, 1024, 67)\n",
      "Model: \"dh\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 2048, 1024,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 1024, 512, 64 9472        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 512, 256, 64) 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 512, 256, 64) 4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 512, 256, 64) 36928       conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 512, 256, 256 16640       conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 512, 256, 256 16640       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add (TensorFlowOpLa [(None, 512, 256, 25 0           conv2d_3[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 512, 256, 64) 16448       tf_op_layer_add[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 512, 256, 64) 36928       conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 512, 256, 256 16640       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_1 (TensorFlowOp [(None, 512, 256, 25 0           conv2d_7[0][0]                   \n",
      "                                                                 tf_op_layer_add[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 512, 256, 64) 16448       tf_op_layer_add_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 256, 128, 64) 36928       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 256, 128, 256 16640       conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 256, 128, 128 32896       conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 256, 128, 128 147584      conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 256, 128, 512 66048       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 256, 128, 512 131584      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_2 (TensorFlowOp [(None, 256, 128, 51 0           conv2d_13[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 256, 128, 128 65664       tf_op_layer_add_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 256, 128, 128 147584      conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 256, 128, 512 66048       conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_3 (TensorFlowOp [(None, 256, 128, 51 0           conv2d_17[0][0]                  \n",
      "                                                                 tf_op_layer_add_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 256, 128, 128 65664       tf_op_layer_add_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 256, 128, 128 147584      conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 256, 128, 512 66048       conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_4 (TensorFlowOp [(None, 256, 128, 51 0           conv2d_20[0][0]                  \n",
      "                                                                 tf_op_layer_add_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 256, 128, 128 65664       tf_op_layer_add_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 128, 64, 128) 147584      conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 128, 64, 512) 66048       conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 128, 64, 256) 131328      conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 128, 64, 256) 590080      conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 128, 64, 1024 263168      conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 128, 64, 1024 525312      conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_5 (TensorFlowOp [(None, 128, 64, 102 0           conv2d_26[0][0]                  \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 128, 64, 256) 262400      tf_op_layer_add_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 128, 64, 256) 590080      conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 128, 64, 1024 263168      conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_6 (TensorFlowOp [(None, 128, 64, 102 0           conv2d_30[0][0]                  \n",
      "                                                                 tf_op_layer_add_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 128, 64, 256) 262400      tf_op_layer_add_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, 64, 256) 590080      conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 128, 64, 1024 263168      conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_7 (TensorFlowOp [(None, 128, 64, 102 0           conv2d_33[0][0]                  \n",
      "                                                                 tf_op_layer_add_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 128, 64, 256) 262400      tf_op_layer_add_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 128, 64, 256) 590080      conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 128, 64, 1024 263168      conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_8 (TensorFlowOp [(None, 128, 64, 102 0           conv2d_36[0][0]                  \n",
      "                                                                 tf_op_layer_add_7[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 128, 64, 256) 262400      tf_op_layer_add_8[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 128, 64, 256) 590080      conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 128, 64, 1024 263168      conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_9 (TensorFlowOp [(None, 128, 64, 102 0           conv2d_39[0][0]                  \n",
      "                                                                 tf_op_layer_add_8[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 128, 64, 256) 262400      tf_op_layer_add_9[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 64, 32, 256)  590080      conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 64, 32, 1024) 263168      conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 64, 32, 512)  524800      conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 64, 32, 512)  2359808     conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 64, 32, 2048) 1050624     conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 64, 32, 2048) 2099200     conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_10 (TensorFlowO [(None, 64, 32, 2048 0           conv2d_45[0][0]                  \n",
      "                                                                 conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 64, 32, 512)  1049088     tf_op_layer_add_10[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 64, 32, 512)  2359808     conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 64, 32, 2048) 1050624     conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_11 (TensorFlowO [(None, 64, 32, 2048 0           conv2d_49[0][0]                  \n",
      "                                                                 tf_op_layer_add_10[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 64, 32, 512)  1049088     tf_op_layer_add_11[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 64, 32, 512)  2359808     conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 64, 32, 2048) 1050624     conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_12 (TensorFlowO [(None, 64, 32, 2048 0           conv2d_52[0][0]                  \n",
      "                                                                 tf_op_layer_add_11[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 64, 32, 512)  1049088     tf_op_layer_add_12[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 128, 64, 512) 524800      tf_op_layer_add_9[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose (Conv2DTranspo (None, 128, 64, 512) 262656      conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 128, 64, 102 0           conv2d_53[0][0]                  \n",
      "                                                                 conv2d_transpose[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 128, 64, 512) 4719104     tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTrans (None, 256, 128, 512 262656      conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 256, 128, 10 0           tf_op_layer_add_4[0][0]          \n",
      "                                                                 conv2d_transpose_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 256, 128, 256 2359552     tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTrans (None, 512, 256, 256 65792       conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_2 (TensorFlo [(None, 512, 256, 51 0           tf_op_layer_add_1[0][0]          \n",
      "                                                                 conv2d_transpose_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 512, 256, 128 589952      tf_op_layer_concat_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_3 (Conv2DTrans (None, 1024, 512, 12 16512       conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_3 (TensorFlo [(None, 1024, 512, 1 0           conv2d[0][0]                     \n",
      "                                                                 conv2d_transpose_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 1024, 512, 64 110656      tf_op_layer_concat_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_4 (Conv2DTrans (None, 2048, 1024, 6 4160        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_4 (TensorFlo [(None, 2048, 1024,  0           input_1[0][0]                    \n",
      "                                                                 conv2d_transpose_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 2048, 1024, 3 19328       tf_op_layer_concat_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 2048, 1024, 2 66          conv2d_59[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 33,465,794\n",
      "Trainable params: 33,465,794\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Initializing from scratch\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    localtime = time.asctime()\n",
    "    print(\"Start at {}\".format(localtime))\n",
    "\n",
    "    model = dh(2)\n",
    "    model.summary()\n",
    "\n",
    "    \"\"\" lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.01,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.96,`\n",
    "        staircase=True)\"\"\"\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "    \n",
    "    manager = tf.train.CheckpointManager(\n",
    "        checkpoint, \n",
    "        directory=\"tf_ckpts/{}\".format(localtime), \n",
    "        max_to_keep=5)\n",
    "    checkpoint.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch\")\n",
    "\n",
    "    train_summary_writer = tf.summary.create_file_writer(\n",
    "        f\"logs/{localtime}/train\")\n",
    "    val_summary_writer = tf.summary.create_file_writer(\n",
    "        f\"logs/{localtime}/val\")\n",
    "    \"\"\" epochs=1\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Epoch {}:\".format(epoch))\n",
    "        with train_summary_writer.as_default():\n",
    "            df=train(model, optimizer, train_dl,epoch)\n",
    "        if True:\n",
    "            checkpoint_path = manager.save(optimizer.iterations)\n",
    "            print(\"Model saved to {}\".format(checkpoint_path))\n",
    "            if val_dl is not None:\n",
    "                with val_summary_writer.as_default():\n",
    "                    val(model, val_dl, optimizer.iterations)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(train_dl().as_numpy_iterator())[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.keras.losses.MSE(tf.constant([1,2,3,4]),tf.constant([1,2,4,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[0.2,0.8,0.0, 0.0],[0.1,0.2,0.7, 0.0]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax_cross_entropy_with_logits(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax_cross_entropy_with_logits([[0,1], [1,0]],np.array([[0.9,0.1],[0.1, 0.9]]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax_cross_entropy_with_logits_v2_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array([0.01,1,0.1])\n",
    "y_hat = y.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y*np.log(y_hat) + (1-y)*np.log(1-y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yln(y^)+(1-y)ln(1-y^)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax_cross_entropy_with_logits(np.array([[0.2,0.8,0.0, 0.0],[0.1,0.2,0.7, 0.0]]).T,[[1.,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filename = \"/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/labels/0.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.io.read_file(image_filename)\n",
    "\n",
    "image = tf.io.decode_png(image, channels=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[image.shape[0]//2:image.shape[0]//2+100,500:550,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image.numpy()[:,:,0], \"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(image.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels =np.array([[0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.array([[[.9,.1], [0., 1], [0.9 ,0.1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax_cross_entropy_with_logits(labels=tf.one_hot(labels,2), logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.one_hot(label,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = tf.cast((image[:,:,0]>100), \"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.random.rand(3,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([[0,0,1],\n",
    "         [1,0,1],\n",
    "         [1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.one_hot(label,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.metrics.Mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "a=tf.constant([1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(a.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self,path,resize=(2048,1024),n_classes=2,batch_size=64,repeat=1):\n",
    "        \"\"\" Loads the tf.data.Dataset \n",
    "        Parameters:\n",
    "            -path        : Path of the Folder containing , \n",
    "                            images and labels folder\n",
    "            -resize      : new size of images in a batch.\n",
    "            -n_classes   : number of classes in labels.\n",
    "            -batch_size  : Batch size \n",
    "            -repeat      : The number of times we want data to repeat. e.g if repeat 1 data is iterted once.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        images_path=os.path.join(path,\"images\")\n",
    "        labels_path=os.path.join(path,\"labels\")\n",
    "\n",
    "        img_paths, label_paths = read_img_paths_and_labels(\n",
    "            images_path, \n",
    "            labels_path)\n",
    "        self.resize = resize\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.size = len(img_paths)\n",
    "\n",
    "\n",
    "        ds = tf.data.Dataset.from_tensor_slices((img_paths, label_paths))\n",
    "        ds = ds.map(self._decode_and_resize)\n",
    "        ds = ds.apply(tf.data.experimental.ignore_errors())\n",
    "        ds = ds.batch(batch_size).repeat(repeat)\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "        self.dataset = ds\n",
    "\n",
    "    def _decode_and_resize(self, image_filename, label_filename):\n",
    "        \"\"\" Reads and resize image (64,image_width)\n",
    "            Parameters:\n",
    "                -image_filename : Name of image file.\n",
    "                -label_filename : Name of label file.\n",
    "                \n",
    "            returns image tensor and label tensor.\n",
    "        \"\"\"\n",
    "        image = tf.io.read_file(image_filename)\n",
    "        image = tf.io.decode_png(image, channels=3)\n",
    "        image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        #image = tf.image.resize(image,self.resize)\n",
    "        label = tf.io.read_file(label_filename)\n",
    "        label = tf.io.decode_png(label, channels=3)\n",
    "        #label = tf.image.resize(label,self.resize)\n",
    "        label = tf.cast((label[:,:,0]>100), tf.int32)\n",
    "        label = tf.one_hot(label, self.n_classes)\n",
    "        return image, label\n",
    "\n",
    "    def __call__(self):\n",
    "        \"\"\"Return tf.data.Dataset.\"\"\"\n",
    "        return self.dataset\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "def read_img_paths_and_labels(images_path, labels_path):\n",
    "    \"\"\"return image filenames  and respective label filenames.\n",
    "       Parameters:\n",
    "           -images_path : Path of the folder of images.\n",
    "           -labels_path : path of the folder of images.\n",
    "    \n",
    "    \"\"\"\n",
    "    img_path=os.listdir(images_path)\n",
    "    img_path.sort()\n",
    "    img_path=[str(os.path.join(images_path,i)) for i in img_path]\n",
    "    label_path=os.listdir(labels_path)\n",
    "    label_path.sort()\n",
    "    label_path=[str(os.path.join(labels_path,i)) for i in label_path]\n",
    "    \n",
    "    return img_path, label_path\n",
    "\n",
    "train_dl=DataLoader(\"/mnt/sda3/code/vscode/project/DataGenv5/DataGen/eval1/\",batch_size=1)\n",
    "\n",
    "for i in train_dl():\n",
    "    continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tf.image.random_hue(i[1][0],0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(np.array(i[1][0])[:,:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at Thu Oct  1 11:42:05 2020\n"
     ]
    }
   ],
   "source": [
    "localtime = localtime = time.asctime()\n",
    "print(\"Start at {}\".format(localtime))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "    \n",
    "manager = tf.train.CheckpointManager(\n",
    "        checkpoint, \n",
    "        directory=\"tf_ckpts/{}\".format(localtime), \n",
    "        max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128, 64, 512) (None, 128, 64, 512)\n",
      "(None, 128, 64, 1024)\n",
      "(None, 256, 128, 512) (None, 256, 128, 512)\n",
      "(None, 256, 128, 1024)\n",
      "(None, 512, 256, 256) (None, 512, 256, 256)\n",
      "(None, 512, 256, 512)\n",
      "(None, 1024, 512, 64) (None, 1024, 512, 128)\n",
      "(None, 1024, 512, 192)\n",
      "(None, 2048, 1024, 3) (None, 2048, 1024, 64)\n",
      "(None, 2048, 1024, 67)\n"
     ]
    }
   ],
   "source": [
    "model=dh(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(manager.latest_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "if checkpoint.restore(manager.latest_checkpoint):\n",
    "    print(\"yes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
