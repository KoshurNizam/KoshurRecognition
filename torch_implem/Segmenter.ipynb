{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_dir, label_dir, transforms=None):\n",
    "        \n",
    "        self.transforms = transforms\n",
    "        self.input_dir = input_dir\n",
    "        self.label_dir = label_dir\n",
    "        \n",
    "        self.input_files = sorted(os.listdir(input_dir))\n",
    "        self.label_files = sorted(os.listdir(label_dir))\n",
    "        \n",
    "        \n",
    "        assert self.input_files == self.label_files\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        input_image = Image.open(os.path.join(self.input_dir, self.input_files[idx]))\n",
    "        input_image = input_image.resize((1024,1536))\n",
    "        label_image = Image.open(os.path.join(self.label_dir, self.label_files[idx]))\n",
    "        label_image = label_image.resize((1024, 1536))\n",
    "        \n",
    "        \n",
    "        if self.transforms is not None:\n",
    "#             print(\"here\")\n",
    "            input_image, label_image = self.transforms((input_image, label_image))\n",
    "#             print(type(input_image), label_image)\n",
    "            \n",
    "#         print(label_image.shape)\n",
    "        label_image = (label_image[0,:,:]>(50/255))*1\n",
    "#         print(label_image.shape)\n",
    "#         label_image = F.one_hot(label_image).transpose(1,2).transpose(0,1)\n",
    "#         print(label_image.shape)\n",
    "        \n",
    "        \n",
    "        return input_image, label_image\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleRandomRotation(transforms.RandomRotation):\n",
    "    def __init__(self, degrees, resample=False, expand=False, center=None, fill=None):\n",
    "        super(MultipleRandomRotation, self).__init__(degrees, resample, expand, center, fill)\n",
    "        \n",
    "    def __call__(self, images):\n",
    "        \n",
    "        if self.fill is None:\n",
    "            self.fill = [None]*len(images)\n",
    "        \n",
    "        angle = self.get_params(self.degrees) \n",
    "        return [TF.rotate(img, angle, self.resample, self.expand, self.center,\n",
    "                          self.fill[i]) for i,img in enumerate(images)]\n",
    "    \n",
    "class MultipleColorJitter(transforms.ColorJitter):\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0, n_max=None):\n",
    "        super(MultipleColorJitter, self).__init__(brightness, contrast, saturation, hue)\n",
    "        \n",
    "        self.n_max = n_max\n",
    "        \n",
    "        \n",
    "    def __call__(self, images):\n",
    "        transform = self.get_params(self.brightness, self.contrast,\n",
    "                                    self.saturation, self.hue)\n",
    "        \n",
    "        if self.n_max is None:\n",
    "            self.n_max = len(images)\n",
    "            \n",
    "#         print(self.n_max)\n",
    "        \n",
    "        out = [transform(images[i]) for i in range(self.n_max)]\n",
    "        out.extend(images[self.n_max:])\n",
    "        return out\n",
    "    \n",
    "class MultipleToTensor(transforms.ToTensor):\n",
    "    def __init__(self):\n",
    "        super(MultipleToTensor, self).__init__()\n",
    "    def __call__(self, images):\n",
    "        return [TF.to_tensor(img) for img in images]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.models.resnet import ResNet, Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    \"\"\"just to delete some layers in pretrained models\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedBottleneck(Bottleneck):\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None, is_last=False):\n",
    "        \n",
    "        super(ModifiedBottleneck, self).__init__(inplanes, planes, stride, downsample, groups,\n",
    "                 base_width, dilation, norm_layer)\n",
    "        \n",
    "        self.is_last = is_last\n",
    "        \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        if not self.is_last:\n",
    "            out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer(nn.Module):\n",
    "    def __init__(self, block, inplanes, planes, blocks, sec_output_block=None, strides=None, connect_last=False):\n",
    "        super(ResNetLayer, self).__init__()\n",
    "        \n",
    "\n",
    "        if strides is None:\n",
    "            strides = [1]*blocks\n",
    "            \n",
    "        self.blocks = blocks\n",
    "        self.sec_output_block = sec_output_block\n",
    "        planes= int(planes/block.expansion)\n",
    "        \n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, planes * block.expansion, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(planes * block.expansion, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "        \n",
    "        self.add_module(\"0\",block(inplanes, planes, strides[0], downsample))\n",
    "        inplanes = int(planes * block.expansion)\n",
    "        \n",
    "        for i in range(1, blocks-1):\n",
    "            self.add_module(str(i),block(inplanes, planes, strides[i]))\n",
    "        i+=1\n",
    "        if connect_last:\n",
    "            is_last=False\n",
    "        else:\n",
    "            is_last = True\n",
    "        self.add_module(str(i),block(inplanes, planes, strides[i], is_last=is_last))\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(self.blocks):\n",
    "            block = getattr(self, str(i))\n",
    "            x = block(x)\n",
    "#             print(x.shape)\n",
    "#             print(self.sec_output_block, i)\n",
    "            if i==self.sec_output_block:\n",
    "#                 print(\"in\")\n",
    "                sec_output = x.clone()\n",
    "        if self.sec_output_block is not None:\n",
    "            return x, sec_output\n",
    "        return x\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyResNet(nn.Module):\n",
    "    def __init__(self, block, layers, strides=None, norm_layer=None, sec_output_blocks=None):\n",
    "        super(MyResNet, self).__init__()\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if strides is None:\n",
    "            strides[None]*len(layers)\n",
    "        if sec_output_blocks is None:\n",
    "            sec_output_blocks = [None]*layers\n",
    "            \n",
    "            \n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = ResNetLayer(block, self.inplanes, self.inplanes*block.expansion, layers[0],\n",
    "                                 strides=strides[0], sec_output_block=sec_output_blocks[0])\n",
    "        \n",
    "        self.inplanes = self.inplanes*block.expansion\n",
    "        \n",
    "        \n",
    "        self.layer2 = ResNetLayer(block, self.inplanes, self.inplanes*block.expansion//2, layers[1],\n",
    "                                 strides=strides[1], sec_output_block=sec_output_blocks[1])\n",
    "        \n",
    "        self.inplanes = self.inplanes*block.expansion//2\n",
    "        \n",
    "        self.layer3 = ResNetLayer(block, self.inplanes, self.inplanes*block.expansion//2, layers[2],\n",
    "                                 strides=strides[2], sec_output_block=sec_output_blocks[2])\n",
    "        \n",
    "        self.inplanes = self.inplanes*block.expansion//2\n",
    "        \n",
    "        self.layer4 = ResNetLayer(block, self.inplanes, self.inplanes*block.expansion//2, layers[3],\n",
    "                                 strides=strides[3], sec_output_block=sec_output_blocks[3])\n",
    "    def forward(self, x):\n",
    "        sec_outputs = []\n",
    "    \n",
    "        \n",
    "        sec_outputs.append(x.clone())\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        \n",
    "        sec_outputs.append(x.clone())\n",
    "        \n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x, sec = self.layer1(x)\n",
    "        sec_outputs.append(sec)\n",
    "        \n",
    "        x, sec = self.layer2(x)\n",
    "        sec_outputs.append(sec)\n",
    "        \n",
    "        x, sec = self.layer3(x)\n",
    "        sec_outputs.append(sec)\n",
    "        \n",
    "        x = self.layer4(x)\n",
    "\n",
    "        return x, sec_outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.rand(1,3,1537,1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1537//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = nn.MaxPool2d(2, ceil_mode=True)(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 769, 512])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1538, 1024])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.UpsamplingNearest2d(scale_factor=2)(mp).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.ConvTranspose2d(3,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpScalerUnit(nn.Module):\n",
    "    def __init__(self, n_current, n_output, n_copy):\n",
    "        super(UpScalerUnit, self).__init__()\n",
    "        self.upsample = nn.UpsamplingNearest2d(scale_factor=2)\n",
    "        self.conv = nn.Conv2d(n_current+n_copy, n_output, kernel_size=(3,3), padding=1, bias=False)\n",
    "        \n",
    "    def forward(self, x, copy):\n",
    "        \n",
    "#         print(x.shape, copy.shape)\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat((copy, x), dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpScaler(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(UpScaler, self).__init__()\n",
    "        \n",
    "        self.upscale1 = UpScalerUnit(512, 512, 512)\n",
    "        \n",
    "        self.upscale2 = UpScalerUnit(512, 256, 512)\n",
    "        \n",
    "        self.upscale3 = UpScalerUnit(256, 128, 256)\n",
    "        \n",
    "        self.upscale4 = UpScalerUnit(128, 64, 64)\n",
    "        \n",
    "        self.upscale5 = UpScalerUnit(64, 32, 3)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, n_classes, kernel_size=1, bias=False)\n",
    "        \n",
    "    def forward(self, x, copies):\n",
    "        \n",
    "        x = self.upscale1(x, copies.pop(-1))\n",
    "        x = self.upscale2(x, copies.pop(-1))\n",
    "        x = self.upscale3(x, copies.pop(-1))\n",
    "        x = self.upscale4(x, copies.pop(-1))\n",
    "        x = self.upscale5(x, copies.pop(-1))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Segmenter, self).__init__()\n",
    "        \n",
    "        self.downsampler = MyResNet(ModifiedBottleneck, [3, 4, 6, 3], \n",
    "                                      strides=[[1,1,2],[1,1,1,2],[1,1,1,1,1,2], None],\n",
    "                                      sec_output_blocks=[1, 2, 4, None])\n",
    "        \n",
    "\n",
    "        \n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        resnet.avgpool = Identity()\n",
    "        resnet.fc = Identity()\n",
    "        \n",
    "        \n",
    "        print(self.downsampler.load_state_dict(resnet.state_dict()))\n",
    "        for param in self.downsampler.parameters():\n",
    "            param.requires_grad = False        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1024, 512, kernel_size=(1,1), bias=False)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(2048, 512, kernel_size=(1,1), bias=False)\n",
    "        \n",
    "        self.upscaler = UpScaler(2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x, sec_outputs = self.downsampler(x)\n",
    "#         print(\"x\", x.shape)\n",
    "        sec_outputs[-1] = self.conv1(sec_outputs[-1])\n",
    "#         for j in sec_outputs:\n",
    "#             print(j.shape)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "#         print(\"x\", x.shape)\n",
    "        x = self.upscaler(x, sec_outputs)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "├─MyResNet: 1-1                               [-1, 2048, 48, 32]        --\n",
      "|    └─Conv2d: 2-1                            [-1, 64, 768, 512]        (9,408)\n",
      "|    └─BatchNorm2d: 2-2                       [-1, 64, 768, 512]        (128)\n",
      "|    └─ReLU: 2-3                              [-1, 64, 768, 512]        --\n",
      "|    └─MaxPool2d: 2-4                         [-1, 64, 384, 256]        --\n",
      "|    └─ResNetLayer: 2-5                       [-1, 256, 192, 128]       --\n",
      "|    |    └─ModifiedBottleneck: 3-1           [-1, 256, 384, 256]       (75,008)\n",
      "|    |    └─ModifiedBottleneck: 3-2           [-1, 256, 384, 256]       (70,400)\n",
      "|    |    └─ModifiedBottleneck: 3-3           [-1, 256, 192, 128]       (70,400)\n",
      "|    └─ResNetLayer: 2-6                       [-1, 512, 96, 64]         --\n",
      "|    |    └─ModifiedBottleneck: 3-4           [-1, 512, 192, 128]       (379,392)\n",
      "|    |    └─ModifiedBottleneck: 3-5           [-1, 512, 192, 128]       (280,064)\n",
      "|    |    └─ModifiedBottleneck: 3-6           [-1, 512, 192, 128]       (280,064)\n",
      "|    |    └─ModifiedBottleneck: 3-7           [-1, 512, 96, 64]         (280,064)\n",
      "|    └─ResNetLayer: 2-7                       [-1, 1024, 48, 32]        --\n",
      "|    |    └─ModifiedBottleneck: 3-8           [-1, 1024, 96, 64]        (1,512,448)\n",
      "|    |    └─ModifiedBottleneck: 3-9           [-1, 1024, 96, 64]        (1,117,184)\n",
      "|    |    └─ModifiedBottleneck: 3-10          [-1, 1024, 96, 64]        (1,117,184)\n",
      "|    |    └─ModifiedBottleneck: 3-11          [-1, 1024, 96, 64]        (1,117,184)\n",
      "|    |    └─ModifiedBottleneck: 3-12          [-1, 1024, 96, 64]        (1,117,184)\n",
      "|    |    └─ModifiedBottleneck: 3-13          [-1, 1024, 48, 32]        (1,117,184)\n",
      "|    └─ResNetLayer: 2-8                       [-1, 2048, 48, 32]        --\n",
      "|    |    └─ModifiedBottleneck: 3-14          [-1, 2048, 48, 32]        (6,039,552)\n",
      "|    |    └─ModifiedBottleneck: 3-15          [-1, 2048, 48, 32]        (4,462,592)\n",
      "|    |    └─ModifiedBottleneck: 3-16          [-1, 2048, 48, 32]        (4,462,592)\n",
      "├─Conv2d: 1-2                                 [-1, 512, 96, 64]         524,288\n",
      "├─Conv2d: 1-3                                 [-1, 512, 48, 32]         1,048,576\n",
      "├─UpScaler: 1-4                               [-1, 2, 1536, 1024]       --\n",
      "|    └─UpScalerUnit: 2-9                      [-1, 512, 96, 64]         --\n",
      "|    |    └─UpsamplingNearest2d: 3-17         [-1, 512, 96, 64]         --\n",
      "|    |    └─Conv2d: 3-18                      [-1, 512, 96, 64]         4,718,592\n",
      "|    └─UpScalerUnit: 2-10                     [-1, 256, 192, 128]       --\n",
      "|    |    └─UpsamplingNearest2d: 3-19         [-1, 512, 192, 128]       --\n",
      "|    |    └─Conv2d: 3-20                      [-1, 256, 192, 128]       2,359,296\n",
      "|    └─UpScalerUnit: 2-11                     [-1, 128, 384, 256]       --\n",
      "|    |    └─UpsamplingNearest2d: 3-21         [-1, 256, 384, 256]       --\n",
      "|    |    └─Conv2d: 3-22                      [-1, 128, 384, 256]       589,824\n",
      "|    └─UpScalerUnit: 2-12                     [-1, 64, 768, 512]        --\n",
      "|    |    └─UpsamplingNearest2d: 3-23         [-1, 128, 768, 512]       --\n",
      "|    |    └─Conv2d: 3-24                      [-1, 64, 768, 512]        110,592\n",
      "|    └─UpScalerUnit: 2-13                     [-1, 32, 1536, 1024]      --\n",
      "|    |    └─UpsamplingNearest2d: 3-25         [-1, 64, 1536, 1024]      --\n",
      "|    |    └─Conv2d: 3-26                      [-1, 32, 1536, 1024]      19,296\n",
      "|    └─Conv2d: 2-14                           [-1, 2, 1536, 1024]       64\n",
      "===============================================================================================\n",
      "Total params: 32,878,560\n",
      "Trainable params: 9,370,528\n",
      "Non-trainable params: 23,508,032\n",
      "Total mult-adds (G): 321.63\n",
      "-----------------------------------------------------------------------------------------------\n",
      "Input size (MB): 18.00\n",
      "Forward/backward pass size (MB): 4512.00\n",
      "Params size (MB): 125.42\n",
      "Estimated Total Size (MB): 4655.42\n",
      "-----------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-----------------------------------------------------------------------------------------------\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "├─MyResNet: 1-1                               [-1, 2048, 48, 32]        --\n",
       "|    └─Conv2d: 2-1                            [-1, 64, 768, 512]        (9,408)\n",
       "|    └─BatchNorm2d: 2-2                       [-1, 64, 768, 512]        (128)\n",
       "|    └─ReLU: 2-3                              [-1, 64, 768, 512]        --\n",
       "|    └─MaxPool2d: 2-4                         [-1, 64, 384, 256]        --\n",
       "|    └─ResNetLayer: 2-5                       [-1, 256, 192, 128]       --\n",
       "|    |    └─ModifiedBottleneck: 3-1           [-1, 256, 384, 256]       (75,008)\n",
       "|    |    └─ModifiedBottleneck: 3-2           [-1, 256, 384, 256]       (70,400)\n",
       "|    |    └─ModifiedBottleneck: 3-3           [-1, 256, 192, 128]       (70,400)\n",
       "|    └─ResNetLayer: 2-6                       [-1, 512, 96, 64]         --\n",
       "|    |    └─ModifiedBottleneck: 3-4           [-1, 512, 192, 128]       (379,392)\n",
       "|    |    └─ModifiedBottleneck: 3-5           [-1, 512, 192, 128]       (280,064)\n",
       "|    |    └─ModifiedBottleneck: 3-6           [-1, 512, 192, 128]       (280,064)\n",
       "|    |    └─ModifiedBottleneck: 3-7           [-1, 512, 96, 64]         (280,064)\n",
       "|    └─ResNetLayer: 2-7                       [-1, 1024, 48, 32]        --\n",
       "|    |    └─ModifiedBottleneck: 3-8           [-1, 1024, 96, 64]        (1,512,448)\n",
       "|    |    └─ModifiedBottleneck: 3-9           [-1, 1024, 96, 64]        (1,117,184)\n",
       "|    |    └─ModifiedBottleneck: 3-10          [-1, 1024, 96, 64]        (1,117,184)\n",
       "|    |    └─ModifiedBottleneck: 3-11          [-1, 1024, 96, 64]        (1,117,184)\n",
       "|    |    └─ModifiedBottleneck: 3-12          [-1, 1024, 96, 64]        (1,117,184)\n",
       "|    |    └─ModifiedBottleneck: 3-13          [-1, 1024, 48, 32]        (1,117,184)\n",
       "|    └─ResNetLayer: 2-8                       [-1, 2048, 48, 32]        --\n",
       "|    |    └─ModifiedBottleneck: 3-14          [-1, 2048, 48, 32]        (6,039,552)\n",
       "|    |    └─ModifiedBottleneck: 3-15          [-1, 2048, 48, 32]        (4,462,592)\n",
       "|    |    └─ModifiedBottleneck: 3-16          [-1, 2048, 48, 32]        (4,462,592)\n",
       "├─Conv2d: 1-2                                 [-1, 512, 96, 64]         524,288\n",
       "├─Conv2d: 1-3                                 [-1, 512, 48, 32]         1,048,576\n",
       "├─UpScaler: 1-4                               [-1, 2, 1536, 1024]       --\n",
       "|    └─UpScalerUnit: 2-9                      [-1, 512, 96, 64]         --\n",
       "|    |    └─UpsamplingNearest2d: 3-17         [-1, 512, 96, 64]         --\n",
       "|    |    └─Conv2d: 3-18                      [-1, 512, 96, 64]         4,718,592\n",
       "|    └─UpScalerUnit: 2-10                     [-1, 256, 192, 128]       --\n",
       "|    |    └─UpsamplingNearest2d: 3-19         [-1, 512, 192, 128]       --\n",
       "|    |    └─Conv2d: 3-20                      [-1, 256, 192, 128]       2,359,296\n",
       "|    └─UpScalerUnit: 2-11                     [-1, 128, 384, 256]       --\n",
       "|    |    └─UpsamplingNearest2d: 3-21         [-1, 256, 384, 256]       --\n",
       "|    |    └─Conv2d: 3-22                      [-1, 128, 384, 256]       589,824\n",
       "|    └─UpScalerUnit: 2-12                     [-1, 64, 768, 512]        --\n",
       "|    |    └─UpsamplingNearest2d: 3-23         [-1, 128, 768, 512]       --\n",
       "|    |    └─Conv2d: 3-24                      [-1, 64, 768, 512]        110,592\n",
       "|    └─UpScalerUnit: 2-13                     [-1, 32, 1536, 1024]      --\n",
       "|    |    └─UpsamplingNearest2d: 3-25         [-1, 64, 1536, 1024]      --\n",
       "|    |    └─Conv2d: 3-26                      [-1, 32, 1536, 1024]      19,296\n",
       "|    └─Conv2d: 2-14                           [-1, 2, 1536, 1024]       64\n",
       "===============================================================================================\n",
       "Total params: 32,878,560\n",
       "Trainable params: 9,370,528\n",
       "Non-trainable params: 23,508,032\n",
       "Total mult-adds (G): 321.63\n",
       "-----------------------------------------------------------------------------------------------\n",
       "Input size (MB): 18.00\n",
       "Forward/backward pass size (MB): 4512.00\n",
       "Params size (MB): 125.42\n",
       "Estimated Total Size (MB): 4655.42\n",
       "-----------------------------------------------------------------------------------------------"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(Segmenter(), torch.randn(1,3,1536,1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, model, optimizer, criterion):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    preds = model(x)\n",
    "    \n",
    "    loss = criterion(preds, y).mean()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def validate(x , y, model, criterion):\n",
    "    \n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    preds = model(x)\n",
    "    \n",
    "    loss = criterion(preds, y).mean()\n",
    "    \n",
    "    return loss.item()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dddacd8281fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#GPU training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #GPU training\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Segmenter()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.Tensor([1,900]),reduction=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([MultipleRandomRotation(10, fill=(255,0)),\n",
    "                                MultipleColorJitter(brightness=0.3, contrast=0.3, n_max=1),\n",
    "                                MultipleToTensor(),\n",
    "                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../side_utils/DataGen/baseline_data/\"\n",
    "dataset = MyDataset(DATA_PATH+\"originals\", DATA_PATH+\"labels\", transforms=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(dataset,batch_size=2,num_workers=0, shuffle=True)\n",
    "val_dl = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (1024,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-72645ec602f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.transpose(0,1).transpose(1,2).numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1429\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1431\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5517\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5519\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5520\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5521\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    706\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 707\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (1024,) for image data"
     ]
    }
   ],
   "source": [
    "for x,y in train_dl:\n",
    "    break\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "f, (ax1, ax2) = plt.subplots(1,2, figsize=(24,12))\n",
    "ax1.imshow(x[0].transpose(0,1).transpose(1,2).numpy())\n",
    "torch\n",
    "ax2.imshow(y[0])#.transpose(0,1).transpose(1,2).numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d6192df959c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print_every = 300\n",
    "\n",
    "i=0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for x, y in train_dl:\n",
    "        \n",
    "        err = train(x, y, model, optimizer, criterion)\n",
    "        \n",
    "        total_loss+=err\n",
    "        \n",
    "        if i%print_every==0:\n",
    "            print(\"current Error: \", err)\n",
    "            \n",
    "        i+=1\n",
    "    \n",
    "    print(\"Epoch\", epoch, \"Avg Train Error: \", total_loss/len(train_dl))\n",
    "    \n",
    "    if val_dl is not None:\n",
    "        model.eval()\n",
    "\n",
    "        eval_loss = 0\n",
    "\n",
    "        for x, y in val_dl:\n",
    "\n",
    "            err = validate(x, y, model, criterion)\n",
    "\n",
    "            eval_loss += err\n",
    "\n",
    "        print(\"Epoch\", epoch, \"Avg Val Error: \", eval_loss/len(val_dl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
