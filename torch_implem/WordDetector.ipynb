{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sn8jLInlQszZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import optim\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "y_YYcYDZ5uEZ",
    "outputId": "ab2d897d-6c4c-4d8c-bd31-ce6f5531168d"
   },
   "outputs": [],
   "source": [
    "## uncomment if using colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Hct2dMiKyKJ0",
    "outputId": "4436b008-7cb5-4059-899c-3ee64943b974"
   },
   "outputs": [],
   "source": [
    "## uncomment if using colab\n",
    "#! unzip \"drive/My Drive/Kmodel/Copy of Test.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cITuCk2sQszm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #GPU training\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ARaEuNUPQszu"
   },
   "outputs": [],
   "source": [
    "SOS = 0 #start of sequence\n",
    "EOS = 1 #end of sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hvo36g--g58"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "files = os.listdir(\"../side_utils/DataGen/Data/WordPics/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "taDO56RJ-pJN"
   },
   "outputs": [],
   "source": [
    "files.sort(key=lambda x:int(x.split(\".\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CbcvyWQ0Qsz1"
   },
   "outputs": [],
   "source": [
    "with open(\"../side_utils/DataGen/Data/labels.txt\", encoding=\"utf8\") as f:\n",
    "    chars = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dU-tY608-45u"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#check if file names and labels are same\n",
    "assert (np.array([int(i.split(\":\")[0]) for i in chars]) == np.array([int(i.split(\".\")[0]) for i in files])).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "oHeovauMQsz8",
    "outputId": "cb427c71-5b63-424b-a884-f08de7923da0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['محمد',\n",
       " 'پوٚت',\n",
       " 'میلنہٕ',\n",
       " 'شفا',\n",
       " 'گواطمینانِ',\n",
       " 'فلک',\n",
       " 'قدر،',\n",
       " 'بۅڈ',\n",
       " 'تا',\n",
       " 'ووتھ']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(map(lambda x:x.split(\":\")[-1].strip(), chars))\n",
    "chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPzrvogyQs0D"
   },
   "outputs": [],
   "source": [
    "chars = list(set(\"\".join(chars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRoxkRkAQs0J"
   },
   "outputs": [],
   "source": [
    "index2word = dict(enumerate(chars,2)) # start from 2, because 0 and 1 are reserved\n",
    "index2word[0] = \"SOS\"\n",
    "index2word[1] = \"EOS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADpOHFBHQs0R"
   },
   "outputs": [],
   "source": [
    "word2index = {v:k for k,v in index2word.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JUfbb83SYddH",
    "outputId": "333b943c-5815-4a69-da31-6e8c4fe7b1b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Ckte4NqY2NP"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"../side_utils/DataGen/Data/train_data\", exist_ok=True)\n",
    "os.makedirs(\"../side_utils/DataGen/Data/val_data\", exist_ok=True)\n",
    "os.makedirs(\"../side_utils/DataGen/Data/test_data\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bk4ENO2aZBcO"
   },
   "outputs": [],
   "source": [
    "train_size = 20000\n",
    "val_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6s7tkSnZNaC"
   },
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lut2C3agZQEI"
   },
   "outputs": [],
   "source": [
    "f1 = open(\"../side_utils/DataGen/Data/labels.txt\", encoding=\"utf8\")\n",
    "f2 = open(\"../side_utils/DataGen/Data/labels_train.txt\", \"a\", encoding=\"utf8\")\n",
    "\n",
    "for file in files[:train_size]:\n",
    "    shutil.copyfile(\"../side_utils/DataGen/Data/WordPics/\"+file,\"../side_utils/DataGen/Data/train_data/\"+file)\n",
    "\n",
    "    f2.write(f1.readline())\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kOMYBmZHawls"
   },
   "outputs": [],
   "source": [
    "# f1 = open(\"labels.txt\", encoding=\"utf8\")\n",
    "f3 = open(\"../side_utils/DataGen/Data/labels_val.txt\", \"a\", encoding=\"utf8\")\n",
    "\n",
    "for file in files[train_size:train_size+val_size]:\n",
    "    shutil.copyfile(\"../side_utils/DataGen/Data/WordPics/\"+file,\"../side_utils/DataGen/Data/val_data/\"+file)\n",
    "\n",
    "    f3.write(f1.readline())\n",
    "f3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qFaxrkwEbmi0"
   },
   "outputs": [],
   "source": [
    "# f1 = open(\"labels.txt\", encoding=\"utf8\")\n",
    "f4 = open(\"../side_utils/DataGen/Data/labels_test.txt\", \"a\", encoding=\"utf8\")\n",
    "\n",
    "for file in files[train_size+val_size:]:\n",
    "    shutil.copyfile(\"../side_utils/DataGen/Data/WordPics/\"+file,\"../side_utils/DataGen/Data/test_data/\"+file)\n",
    "\n",
    "    f4.write(f1.readline())\n",
    "f4.close()\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbn_eJrXQs0m"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "class WordDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root_folder, labels_path, transform=None):\n",
    "        self.transform = transform\n",
    "        self.root_folder = root_folder\n",
    "        self.files = os.listdir(root_folder)\n",
    "        self.files.sort(key=lambda x:int(x.split(\".\")[0]))\n",
    "        with open(labels_path) as f:\n",
    "            self.labels = f.readlines()\n",
    "            self.labels = self.labels\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "#         image = cv2.imread(os.path.join(self.root_folder, self.files[idx]), 0)\n",
    "        image = Image.open(os.path.join(self.root_folder, self.files[idx]))\n",
    "#         image = (255-image[:,::-1])/255\n",
    "#         label_no = self.files[idx].split(\".\")[0]\n",
    "        label = self.labels[idx].split(\":\")[-1].strip()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FdMoVHDVQs0s"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Grayscale(3),\n",
    "                                transforms.RandomRotation(15, fill=255), # add an issue in torchvision\n",
    "                                transforms.RandomAffine(20,fillcolor=(255,255,255)),\n",
    "                                transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
    "                                transforms.ToTensor(),])\n",
    "#                                 transforms.Normalize(mean=[0.06333], std=[1.4219])])\n",
    "\n",
    "transform2 = transforms.Compose([transforms.Grayscale(3),\n",
    "                                transforms.ToTensor(),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9pI7I4MdQs0x"
   },
   "outputs": [],
   "source": [
    "# dataset = WordDataset(\"./WordPics\",\"./labels.txt\", transform)\n",
    "train_dataset = WordDataset(\"../side_utils/DataGen/Data/train_data\",\n",
    "                            \"../side_utils/DataGen/Data/labels_train.txt\", transform)\n",
    "val_dataset = WordDataset(\"../side_utils/DataGen/Data/val_data\",\n",
    "                          \"../side_utils/DataGen/Data/labels_val.txt\", transform)\n",
    "test_dataset = WordDataset(\"../side_utils/DataGen/Data/test_data\",\n",
    "                           \"../side_utils/DataGen/Data/labels_test.txt\", transform2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HOZdbF-JQs02"
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dataset,batch_size=1,num_workers=0, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset,batch_size=1,num_workers=0, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset,batch_size=1,num_workers=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzFTk248Qs1G"
   },
   "source": [
    "> Think of some way to put an **EOS** in the encoder. While i currently think it is not necessary, or that encoder and decoder need not have the same **EOS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPLkpDvpjJz4"
   },
   "source": [
    "Check if a random image is correctly labelled with the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "colab_type": "code",
    "id": "qENN8h0RpULy",
    "outputId": "cdff4431-eaec-4da3-e4c9-fc95caf2d818"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('سپدٕۍمٕتۍ،',)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f53480d20d0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "iterator = iter(train_dl) # change to train_dl or val_dl to check in those datasets\n",
    "image, label = iterator.next()\n",
    "import matplotlib.pyplot as plt\n",
    "print(label)\n",
    "plt.imshow(image[0].transpose(0,1).transpose(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PqJ4xsSJf9sm"
   },
   "source": [
    "# Models\n",
    "\n",
    "The model architechture is organized into three modules:\n",
    "1. `ImgEncoder`: A convolutional network. \n",
    "  -  It will take input of an image of `[1, 60, x]` (channels, height, width) and produce the output of `[128, 12, x']`, which is then flattened out to `[1536, x']`.\n",
    "\n",
    "2. `Encoder(input_size, hidden_size, n_layers)`: \n",
    "  - It has a linear layer followed by a GRU layer.\n",
    "  - It takes a vector of `[1, input_size]` and the linear converts into `[1, hidden_size]` vector. \n",
    "  - Then the `[1, hidden_size]` vector is passed via a GRU with `n_layers`.\n",
    "  - The hidden input to GRU is initialized as zeros.\n",
    "\n",
    "3. `DecoderRNN(hidden_size, output_size, n_layers)`: \n",
    "   - It has an embedding layer giving out `hidden_size` length of vectors.\n",
    "   - Then a GRU layer with `hidden_size` output and `n_layers`.\n",
    "   - Then a linear layer of `output_size` nodes.\n",
    "   - Then a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ozSMbqb1GPmU"
   },
   "outputs": [],
   "source": [
    "class ImgEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImgEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,16,kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(16,32,kernel_size=3)        \n",
    "        self.conv3 = nn.Conv2d(32,64,kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(64,128,kernel_size=3)\n",
    " \n",
    "        \n",
    "        self.pool = nn.MaxPool2d((2,2))\n",
    "        self.flatten = nn.Flatten(end_dim=-2)\n",
    "        \n",
    "        \n",
    "        ,\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        x = self.pool(x)\n",
    "       \n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJ_zlzhLQs0b"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    " \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size,hidden_size)\n",
    "        \n",
    "        \n",
    "#         self.em = nn.Embedding(input_size,hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size,hidden_size,num_layers=n_layers)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        output = F.relu(self.fc1(input)).view(1,1,-1)\n",
    "        \n",
    "#         embedded = self.em(input).view(1,1,-1)\n",
    "#         output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.n_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6VkRsU0Qs0h"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size, n_layers):\n",
    "        \n",
    "        super(DecoderRNN, self).__init__()\n",
    " \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.em = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers=n_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        out = self.em(input).view(1,1,-1)\n",
    "        out = F.relu(out)\n",
    "        out, hidden = self.gru(out, hidden)\n",
    "        out = self.softmax(self.out(out[0]))\n",
    "        return out, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.n_layers,1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U0MbPAMoJJyF"
   },
   "source": [
    "We will add the `EOS` to every label to denote the end of that vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjWQpIy4Qs1B"
   },
   "outputs": [],
   "source": [
    "def get_target(word):\n",
    "    \"\"\"This function converts a label into a target. It also adds `EOS` at the end.\"\"\"\n",
    "    t = [word2index[i] for i in word]\n",
    "    t.append(EOS)\n",
    "    return torch.tensor(t, dtype=torch.long, device=device).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAt2W1y6Qs1l"
   },
   "outputs": [],
   "source": [
    "def train(image, label, conv, enc, dec, conv_optim, enc_optim, dec_optim, criterion):\n",
    "    \n",
    "    \n",
    "    loss=0 \n",
    "\n",
    "\n",
    "    # set gradients of every optimizers to be zero\n",
    "    conv_optim.zero_grad()\n",
    "    enc_optim.zero_grad()\n",
    "    dec_optim.zero_grad()\n",
    " \n",
    "    image = image.to(device)\n",
    "    conv_output = conv(image) # move through conv part\n",
    "    # conv_output shape will be [1,1536, x']\n",
    "    \n",
    "    \n",
    "    enc_hidden = enc.initHidden() \n",
    "    \n",
    "\n",
    "    # pass each of the [1, 1536] vectors to encoder\n",
    "    for idx in range(conv_output.shape[2]): # for every \n",
    "        enc_out, enc_hidden = enc(conv_output[:,:,idx], enc_hidden)\n",
    " \n",
    "    target = get_target(label[0]) # makes a tensor and adds EOS\n",
    "    target_len = len(target)\n",
    "    target.to(device)\n",
    " \n",
    " \n",
    "    decoder_input = torch.tensor([[[SOS]]], device=device)\n",
    "\n",
    "    # the output of encoder is now hidden input for decoder\n",
    "    decoder_hidden = enc_hidden \n",
    "\n",
    "\n",
    "    for di in range(target_len):\n",
    "        decoder_output, decoder_hidden = dec(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1) # get the top output\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    " \n",
    "        loss += criterion(decoder_output, target[di])\n",
    "\n",
    "        # if decoder spits out EOS, stop.\n",
    "        if decoder_input.item() == EOS:\n",
    "            break\n",
    "    \n",
    "    # compute gradients for loss\n",
    "    loss.backward()\n",
    " \n",
    "    # update weights \n",
    "    conv_optim.step()\n",
    "    enc_optim.step()\n",
    "    dec_optim.step()\n",
    "    \n",
    "    return loss.item()/target_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKXixI_l7Vz6"
   },
   "outputs": [],
   "source": [
    "def validate(image, label, conv, enc, dec, crterion):\n",
    "    loss=0\n",
    "\n",
    "    image = image.to(device)\n",
    "    conv_output = conv(image)\n",
    " \n",
    "    enc_hidden = enc.initHidden()\n",
    "    \n",
    "    for idx in range(conv_output.shape[2]):\n",
    "        enc_out, enc_hidden = enc(conv_output[:,:,idx], enc_hidden)\n",
    " \n",
    "    target = get_target(label[0])\n",
    "    target_len = len(target)\n",
    "    target.to(device)\n",
    " \n",
    " \n",
    "    decoder_input = torch.tensor([[[SOS]]], device=device)\n",
    "    decoder_hidden = enc_hidden\n",
    "\n",
    " \n",
    "    for di in range(target_len):\n",
    "        decoder_output, decoder_hidden = dec(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    " \n",
    "        loss += criterion(decoder_output, target[di])\n",
    "        if decoder_input.item() == EOS:\n",
    "            break\n",
    "    \n",
    "    return loss.item()/target_len\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLvoJNkkQs1N"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "conv = ImgEncoder()\n",
    "enc = Encoder(1536,256,3)\n",
    "dec = DecoderRNN(256, len(word2index),3)\n",
    "criterion = nn.NLLLoss()\n",
    "enc_optim = optim.SGD(enc.parameters(), lr=learning_rate, momentum=0.9)\n",
    "dec_optim = optim.SGD(dec.parameters(), lr=learning_rate, momentum=0.9)\n",
    "conv_optim = optim.SGD(conv.parameters(), lr=learning_rate, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xN4WFhYQHY_V",
    "outputId": "df31a469-7918-435b-ef21-0b1b2f59072c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# nn.GRU()\n",
    "# criterion()\n",
    "len(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8NaGhrcyuze"
   },
   "outputs": [],
   "source": [
    "main_path = \"drive/My Drive/Kmodel/\" \n",
    "\n",
    "# conv.load_state_dict(torch.load(main_path+\"conv2.pt\"))\n",
    "# enc.load_state_dict(torch.load(main_path+\"enc2.pt\"))\n",
    "# dec.load_state_dict(torch.load(main_path+\"dec2.pt\"))\n",
    "\n",
    "conv = conv.to(device)\n",
    "enc = enc.to(device)\n",
    "dec = dec.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KvPlTKXJQs1p"
   },
   "outputs": [],
   "source": [
    "n_epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRXN3ZvG9dGd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Error:  4.761396408081055\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-1993661fd252>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         err = train(image, label, conv, enc, dec,\n\u001b[0;32m---> 17\u001b[0;31m               conv_optim, enc_optim, dec_optim, criterion)\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mprint_every\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-d6300f28dcf6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(image, label, conv, enc, dec, conv_optim, enc_optim, dec_optim, criterion)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mdec_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtarget_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print_every = 5000\n",
    "i=0\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # set the models in train mode\n",
    "    conv.train()\n",
    "    enc.train()\n",
    "    dec.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for image, label in train_dl:\n",
    "        if image.shape[3] < 20:\n",
    "            continue\n",
    "        err = train(image, label, conv, enc, dec,\n",
    "              conv_optim, enc_optim, dec_optim, criterion)\n",
    "        total_loss+=err\n",
    "        if i%print_every ==0:\n",
    "            print(\"Current Error: \",err)\n",
    "        i+=1\n",
    "    # if epoch%100==0:\n",
    "    print(\"Epoch\", epoch, \"Avg Train Error: \", total_loss/len(train_dl))\n",
    "    # inp = input(\"Change Learning rate?\")\n",
    "    # if inp.casefold() == 'y':\n",
    "    #   learning_rate = float(input(\"Enter Learning rate\"))\n",
    "    #   enc_optim = optim.Adam(enc.parameters(), lr=learning_rate)#, momentum=0.9)\n",
    "    #   dec_optim = optim.Adam(dec.parameters(), lr=learning_rate)#, momentum=0.9)\n",
    "    #   conv_optim = optim.Adam(conv.parameters(), lr=learning_rate)#, momentum=0.9)\n",
    "\n",
    "    # print(\"saving models\")\n",
    "    # torch.save(conv.state_dict(), main_path+\"conv.pt\")\n",
    "    # torch.save(enc.state_dict(), main_path+\"enc.pt\")\n",
    "    # torch.save(dec.state_dict(), main_path+\"dec.pt\")\n",
    "\n",
    "\n",
    "    ################################################\n",
    "    # evalualtion mode\n",
    "    conv.eval()\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "    eval_loss = 0\n",
    "\n",
    "    for image, label in val_dl:\n",
    "        if image.shape[3] < 20:\n",
    "            continue\n",
    "        err = validate(image, label, conv, enc, dec, criterion)\n",
    "        eval_loss += err\n",
    "    print(\"Epoch\", epoch, \"Avg Val Error: \", eval_loss/len(val_dl))\n",
    "    \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQquqcfKlpTn"
   },
   "source": [
    "Test your model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UjLW31Ymu3Zu"
   },
   "outputs": [],
   "source": [
    "conv.eval()\n",
    "enc.eval()\n",
    "dec.eval()\n",
    "\n",
    "preds = []\n",
    "tars = [] # targets\n",
    "\n",
    "for image, label in test_dl:\n",
    "    if image.shape[3] < 20:\n",
    "        continue\n",
    "\n",
    "    image = image.to(device)\n",
    "\n",
    "    conv_output = conv(image)\n",
    " \n",
    "    enc_hidden = enc.initHidden()\n",
    " \n",
    "    for idx in range(conv_output.shape[2]):\n",
    "        enc_out, enc_hidden = enc(conv_output[:,:,idx], enc_hidden)\n",
    " \n",
    "    target = get_target(label[0])\n",
    "    target_len = len(target)\n",
    "\n",
    "    tars.append(target.detach().cpu().numpy().reshape(1,-1))\n",
    " \n",
    " \n",
    " \n",
    "    decoder_input = torch.tensor([[[SOS]]], device=device)\n",
    "    decoder_hidden = enc_hidden\n",
    " \n",
    "    # print(\"actual: \",label[0], \"predicted: \",end=\"\")\n",
    "    current_pred = []\n",
    "    \n",
    "    while True: # run until you hit EOS\n",
    "        decoder_output, decoder_hidden = dec(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.topk(1)\n",
    "        decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "        current_pred.append(decoder_input.item())\n",
    " \n",
    "        if decoder_input.item() == EOS:\n",
    "                break\n",
    "        # print(index2word.get(decoder_input.item(),\"None\"),end=\"\")\n",
    "    preds.append(np.array(current_pred))\n",
    "\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9YBuNzQ62jp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def levenshtein(seq1, seq2):\n",
    "    size_x = len(seq1) + 1\n",
    "    size_y = len(seq2) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if seq1[x-1] == seq2[y-1]:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    # print (matrix)\n",
    "    return (matrix[size_x - 1, size_y - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QGTBQRB6y4Rz"
   },
   "outputs": [],
   "source": [
    "new_tars = []\n",
    "for e in tars:\n",
    "  try:\n",
    "    len(e.squeeze())\n",
    "    new_tars.append(e.squeeze())\n",
    "  except TypeError:\n",
    "    new_tars.append(np.array([e.squeeze()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kg9Phoke7Qek"
   },
   "outputs": [],
   "source": [
    "dists = [levenshtein(i,j) for i, j in zip(preds, new_tars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJ4n8XQv8GBO"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zgV7NHT48Inv"
   },
   "outputs": [],
   "source": [
    "c = Counter(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "pMP_UBMV8QHu",
    "outputId": "9b838b87-4e31-40c7-b1c7-983eee1928f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0.0: 1049,\n",
       "         1.0: 696,\n",
       "         2.0: 438,\n",
       "         3.0: 210,\n",
       "         4.0: 96,\n",
       "         5.0: 45,\n",
       "         6.0: 18,\n",
       "         7.0: 19,\n",
       "         8.0: 7,\n",
       "         9.0: 5,\n",
       "         10.0: 5,\n",
       "         11.0: 1,\n",
       "         14.0: 2})"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nHuIWKjEsjUV",
    "outputId": "065b6a32-a511-48b2-fef0-22ba14f37d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:  1.388873596909713\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "for image, label in test_dl:\n",
    "    if image.shape[3] < 20:\n",
    "        continue\n",
    "    err = validate(image, label, conv, enc, dec, criterion)\n",
    "    test_loss += err\n",
    "print(\"Test loss: \", eval_loss/len(test_dl))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "OuOGu-pxtIuu",
    "outputId": "5570b7ce-7cc7-4402-ef86-d6e73e0d9375"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2600"
      ]
     },
     "execution_count": 67,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dl)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KaeshurRecognitionModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
